---
title: "Homework 7"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE, error = TRUE,
                      fig.height = 3)
library(tidyverse)
```

# Preliminaries

- This file should be in `STAT240/homework/hw07` on your local computer.

# Problem 1

The weight of adult male grizzly bears in the continental United States is well approximated by a normal distribution with mean 510 pounds and standard deviation 45 pounds. The weight of adult female grizzly bears in the continental United States is well approximated by a normal distribution with mean 315 pounds and standard deviation 37 pounds.

Suppose a male grizzly bear that is 441 pounds is observed. What would be the approximate weight of a female grizzly bear with the same weight percentile as this male grizzly bear?

```{r}
male_mean <- 510
male_sd <- 45
female_mean <- 315
female_sd <- 37

male_weight <- 441
male_percentile <- pnorm(male_weight, mean = male_mean, sd = male_sd)

female_weight <- qnorm(male_percentile, mean = female_mean, sd = female_sd)

female_weight


```


# Problem 2

The lifespan of a certain type of tire is approximately normal, with mean 39 and standard deviation 2.5 (units are in thousands of miles).
  
**(a)** What is the probability that a random tire will last longer than 40,000 miles?

```{r}
mean_lifespan <- 39
sd_lifespan <- 2.5
threshold <- 40

probability_longer_than_40k <- 1 - pnorm(threshold, mean = mean_lifespan, sd = sd_lifespan)

probability_longer_than_40k

```

**(b)** What is the 95th percentile of a single tire's lifespan?

```{r}
mean_lifespan <- 39
sd_lifespan <- 2.5
percentile_95 <- qnorm(0.95, mean = mean_lifespan, sd = sd_lifespan)

percentile_95

```

**(c)** Take a random sample of 4 tires.  What is the probability that the average lifespan of the four tires will be greater than 40,000 miles?

```{r}
mean_lifespan <- 39
sd_lifespan <- 2.5
sample_size <- 4
threshold <- 40

standard_error <- sd_lifespan / sqrt(sample_size)

probability_mean_greater_than_40k <- 1 - pnorm(threshold, mean = mean_lifespan, sd = standard_error)

probability_mean_greater_than_40k

```


**(d)** What is the 95th percentile for the average lifespan of four tires?  
```{r}
mean_lifespan <- 39
sd_lifespan <- 2.5
sample_size <- 4

standard_error <- sd_lifespan / sqrt(sample_size)

percentile_95_avg_lifespan <- qnorm(0.95, mean = mean_lifespan, sd = standard_error)

percentile_95_avg_lifespan

```

**(e)** Explain why the answers to (a) and (b) are so different from the answers to (c) and (d).

> The answers differ because (a) and (b) refer to the lifespan of a single tire, while (c) and (d) involve the average lifespan of a sample of four tires, which has less variability due to the reduced standard error. This makes the distribution of the sample mean narrower, affecting the probabilities and percentiles.


# Problem 3

Suppose you are playing a coin flipping game with a friend, where you suspect the coin your friend provided is not a fair coin.  In fact, you think the probability the coin lands heads is less than 0.5.  To test this, you flip the coin 100 times and observe the coin lands heads 30 times.
  
If you assume the coin is fair (i.e., the probability of the coin landing heads is 0.5), what is the probability of observing 30 heads or fewer?

```{r}
observed_heads <- 30
total_flips <- 100
prob_head <- 0.5

probability_30_or_fewer <- pbinom(observed_heads, size = total_flips, prob = prob_head)

probability_30_or_fewer

```

Calculate the previous probability, but use a normal approximation to achieve a numerical value. What is the error in this approximation?

```{r}
observed_heads <- 30
total_flips <- 100
prob_head <- 0.5

mean_approx <- total_flips * prob_head
sd_approx <- sqrt(total_flips * prob_head * (1 - prob_head))

probability_approx <- pnorm(30.5, mean = mean_approx, sd = sd_approx)
probability_exact <- pbinom(observed_heads, size = total_flips, prob = prob_head)

error <- abs(probability_exact - probability_approx)

list(probability_approx = probability_approx, probability_exact = probability_exact, error = error)

```

How small would $p$ need to be (rounded to the nearest 0.01) for the probability of observing 30 or fewer heads to be at least 0.05?

*Hint: You could determine $p$ by trial and error, but there is also a quicker solution using `tibble()`.*

```{r}
library(tibble)
library(purrr)

observed_heads <- 30
total_flips <- 100

results <- tibble(p = seq(0.01, 0.5, by = 0.01)) %>%
  mutate(probability = map_dbl(p, ~ pbinom(observed_heads, size = total_flips, prob = .x))) %>%
  filter(probability >= 0.05) %>%
  slice(1)

results

```


# Problem 4

The code below does the following: 

- Draw `n` points from a population called `pop`
- Calculate the mean of the `n` points
- Repeat this `iterations` times to get a large number of sample means

```{r, eval = FALSE}
iterations <- 1000
n <- 30
pop <- rnorm(10000, mean = 50, sd = 10)

sample_means <- replicate(iterations, mean(sample(pop, n, replace = TRUE)))

head(sample_means)


```

Perform this operation three times with 10000 iterations each.  Do this with $n = 5$, $n = 10$, and $n = 50$. Use `right_skewed` below as your population, `pop`.

Then, make three density plots to compare the three sampling distributions.  What do you notice as $n$ increases?

```{r}
right_skewed <- c(rep(0, 60), rep(1, 25), rep(2, 10), rep(3, 5))
```

> As n increases, the sampling distributions of the mean become more symmetric and approach a normal distribution, even though the original population is right-skewed. This illustrates the Central Limit Theorem, where larger sample sizes lead to a sampling distribution of the mean that is nearly normal.

# Problem 5

Continue the analysis of the right-skewed population from problem 4.  Now, perform the sampling process three times, each with $n = 50$.  Do this with 50 iterations, 500 iterations, and 50000 iterations.  

Then, make three density plots to compare the three sampling distributions.  What do you notice as the number of iterations increases?

```{r}
right_skewed <- c(rep(0, 60), rep(1, 25), rep(2, 10), rep(3, 5))

```

> With more iterations, the sampling distributions become smoother and more stable, better approximating the true distribution.


# Problem 6

For each of the three prompts, explain which condition will result in a **narrower** confidence interval.  If the confidence level would not change, explain why.

- Having a **larger** confidence level (smaller alpha) versus having a **smaller** confidence level (larger alpha)

> A smaller confidence level (larger alpha) results in a narrower confidence interval because less confidence allows for a smaller range around the estimate, while a higher confidence level requires a wider interval to capture the true parameter more reliably.

- Having **larger** estimation error (a.k.a "sampling error") versus having **smaller** estimation error

> A smaller estimation error results in a narrower confidence interval, as it indicates less variability in the data. Larger estimation error increases variability, requiring a wider interval to account for the increased uncertainty.

- Having a **larger** point estimate versus having a **smaller** point estimate

> 
The size of the point estimate does not affect the width of the confidence interval; it only shifts the interval's position along the number line. The interval width depends on the confidence level and estimation error, not on the estimate's size.

# Problem 7

The code below calculates the Z critical value to build a 95% confidence interval.  Edit the code to instead calculate the critical value for **98%** confidence.

```{r}
qnorm(0.975) # Critical Value for a 95% confidence interval; edit it to produce critical value for a 98% confidence interval.
```

Next, generalize this operation to any confidence level.  Uncomment and edit the code below to return the critical value for a (1-alpha) confidence interval for any choice of alpha.

```{r}
# For example, a 90% CI
alpha <- 0.1

alpha <- 0.1  

critical_value <- qnorm(1 - alpha / 2)
critical_value

```


# Problem 8

For Halloween, you have purchased a big bag of candy made up of equal amounts of KitKats, Hershey's, Almond Joys, and Reese's candy bars.

**(a)** A small child reaches in and pulls out three Reese's candy bars.  What is the probability that all three of their three candies were Reese's by random chance?  (Assume candy bar draws are independent and all have equal probability of being a Reese's.)

```{r}
prob_reeses <- (1/4)^3
prob_reeses

```

**(b)** A different small child reaches in and takes seven Reese's candy bars and one other candy bar.  What is the probability that seven or more of their eight candies were Reese's by random chance?

```{r}
n <- 8
p <- 1/4

prob_seven_or_more <- dbinom(7, size = n, prob = p) + dbinom(8, size = n, prob = p)
prob_seven_or_more

```

**(c)** Intuitively, which of the two situations is more "suspicious"?  Do the probabilities match your intuition?

>The second situation is more suspicious, as itâ€™s far less likely to happen by chance. The probabilities confirm this, showing the second scenario is much more unusual.



